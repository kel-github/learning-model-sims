side.pred = get.ys.control( side.cont, betas, side.mod, side )
side.out = side.pred %>% group_by(pred) %>%
summarise( y = mean(y) )
f_size = 12
lwds  = 2
alph  = 7/10
s.p  = ggplot(side.out, aes( x = pred, y = y ) ) +
geom_point() + geom_line(size=lwds, alpha = alph, group=1,     col=wesanderson::wes_palette("IsleofDogs1")[6]) +
ylab("Pred Y") + xlab("Target | Action side") +
theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),
text=element_text(size=f_size),
axis.text.x = element_text(size=f_size),
axis.text.y = element_text(size=f_size),
legend.position=c(0.85, 0.75))
s.p
side.out = side.pred %>% group_by(pred) %>%
summarise( y = mean(y) )
f_size = 12
lwds  = 2
alph  = 7/10
s.p  = ggplot(side.out, aes( x = pred, y = y ) ) +
geom_point() + geom_line(size=lwds, alpha = alph, group=1,     col=wesanderson::wes_palette("IsleofDogs1")[6]) +
ylab("Pred Y") + xlab("Target | Action side") +
theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),
text=element_text(size=f_size),
axis.text.x = element_text(size=f_size),
axis.text.y = element_text(size=f_size),
legend.position=c(0.85, 0.75))
s.p
? rbeta
# define the model parameters
# I have chosen to randomly sample them, given the prior distributions, taken from Ouder et al, 2013, Neuron
experience_decay_factor = rbeta(1, 1.2, 1.2)
payoff_decay_factor = rbeta(1, 1.2, 1.2)
inverse_temperature = rnorm(1, 0, 10)
? sample
outcomes = as.matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2)
outcome_probs_blue = c(.8, .2) # blue is good
outcome_probs_orange = c(.2, .8) # orange is bad
outcomes = as.matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2)
outcomes
outcomes = as.matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2, ncol = 2)
? as.matrix
outcomes = matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2, ncol = 2)
outcomes
? sample
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, c(.5, .5) )
# define weightings of probability of good or bad outcome for each choice
# assuming choice between two stimuli, a 'blue' and an 'orange'
choices = c("blue", "orange")
i = 1
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, c(.5, .5) )
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, prob = c(.5, .5) )
choice = c() # empty vector for choices
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, prob = c(.5, .5) )
choice[i]
get_trial_outcome <- function( choice, choice_probs ){
# sample the reward distribution lying beneath the choice,
if (choice == "blue"){
idx = 1
} else if (choice == "orange"){
idx = 2
}
outcome = sample(c(1, 0), 1, probs = choice_probs[idx, ])
}
outcomes = c() # empty vector for outcomes
# get trial outcome
outcomes[i] = get_trial_outcome(choice[i], outcomes)
? sample
get_trial_outcome <- function( choice, choice_probs ){
# sample the reward distribution lying beneath the choice,
if (choice == "blue"){
idx = 1
} else if (choice == "orange"){
idx = 2
}
outcome = sample(c(1, 0), 1, prob = choice_probs[idx, ])
}
# get trial outcome
outcomes[i] = get_trial_outcome(choice[i], outcomes)
outcomes
choice == "blue"
choice = sample(c("blue", "orange"), 1, prob = c(.5, .5))
choice
choice = sample(c("blue", "orange"), 10, prob = c(.5, .5))
choice = sample(c("blue", "orange"), 10, replace = TRUE, prob = c(.5, .5))
choice
choice == "orange"
which(choice == "orange")
max(which(idx))
max(which(choice == "orange"))
# compute ewa & action selection
tmp = values(c(max(which(choice == "blue")), max(which(choice == "orange"))
)
)
values
values = c()
# compute ewa & action selection
tmp = values(c(max(which(choice == "blue")), max(which(choice == "orange"))))
# compute ewa & action selection
tmp = values[c(max(which(choice == "blue")), max(which(choice == "orange")))]
tmp
? is.null
values[i] = .5 # because it is 50/50 at this point
# compute ewa & action selection
tmp = values[c(max(which(choice == "blue")), max(which(choice == "orange")))] # this gets the last value for each choice
tmp
choice = c() # empty vector for choices
values = c()
start_values = c(0.5, 0.5) # initialise with a value of 0.5 for each value (50:50) they'll give reward
outcomes = c() # empty vector for outcomes
ntrials = 50 # will do 50 before switching
i
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, prob = c(.5, .5) )
values[i] = .5 # because it is 50/50 at this point
choice
values
choice
max(which(choice == "orange")))
max(which(choice == "orange"))
max(which(choice == "blue"))
values[c(max(which(choice == "blue")), max(which(choice == "orange")))]
# compute ewa & action selection
tmp = values[c(max(which(choice == "blue")), max(which(choice == "orange")))] # this gets the last value for each choice
is.na(tmp)
tmp[is.na(tmp)] = .5 # because not assigned a value yet
tmp
outcome
outcomes
# get trial outcome
outcomes[i] = get_trial_outcome(choice[i], outcomes)
outcomes
i = 2
which(choices == choice[i-1])
weight_for_update = tmp[ which(choices == choice[i-1])   ]
tmp
wight_for_update
weight_for_update
# this code is written for readability over effieciency
rm(list=ls())
ewa_update <- function(experience_decay_factor, payoff_decay_factor, last_experience_weight, last_choice_value, previous_outcome){
# given an experience decay factor, and  a payoff_decay _factor, compute the
# experience weight and the value for the choice, given the value, weight and outcome
# for the previous choice
experience_weight = last_experience_weight * experience_decay_factor + 1
choice_value = (last_choice_value * payoff_decay_factor * last_experience_weight + previous_outcome) / experience_weight
choice_value
}
action_selection <- function( choice_value, other_choice_value, inverse_temperature ){
# compute the probability of choice, given the value for that choice, the value for the other choice, and the
# inverse temperature parameter -
probability_choice = exp( beta*choice_value  ) / sum( exp( beta*choice_value ), exp( beta*other_choice_value ) )
probability_choice
}
get_trial_outcome <- function( choice, choice_probs ){
# sample the reward distribution lying beneath the choice,
if (choice == "blue"){
idx = 1
} else if (choice == "orange"){
idx = 2
}
outcome = sample(c(1, 0), 1, prob = choice_probs[idx, ])
}
experience_decay_factor = rbeta(1, 1.2, 1.2)
payoff_decay_factor = rbeta(1, 1.2, 1.2)
inverse_temperature = rnorm(1, 0, 10)
# define weightings of probability of good or bad outcome for each choice
# assuming choice between two stimuli, a 'blue' and an 'orange'
choices = c("blue", "orange")
outcome_probs_blue = c(.8, .2) # blue is good
outcome_probs_orange = c(.2, .8) # orange is bad
outcomes = matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2, ncol = 2)
# set up empty vectors to collect generated data
choice = c() # empty vector for choices
action_values = c()
experience_weights = c()
outcomes = c() # empty vector for outcomes
ntrials = 50 # will do 50 before switching
ewa_update(experience_decay_factor, payoff_decay_factor, weight_for_update, )
rm(list=ls())
###############################################################################################################################
# define model functions
ewa_update <- function(experience_decay_factor, payoff_decay_factor, last_experience_weight, last_choice_value, previous_outcome){
# given an experience decay factor, and  a payoff_decay _factor, compute the
# experience weight and the value for the choice, given the value, weight and outcome
# for the previous choice
experience_weight = last_experience_weight * experience_decay_factor + 1
choice_value = (last_choice_value * payoff_decay_factor * last_experience_weight + previous_outcome) / experience_weight
choice_value
}
action_selection <- function( choice_value, other_choice_value, inverse_temperature ){
# compute the probability of choice, given the value for that choice, the value for the other choice, and the
# inverse temperature parameter -
probability_choice = exp( beta*choice_value  ) / sum( exp( beta*choice_value ), exp( beta*other_choice_value ) )
probability_choice
}
get_trial_outcome <- function( choice, choice_probs ){
# sample the reward distribution lying beneath the choice,
if (choice == "blue"){
idx = 1
} else if (choice == "orange"){
idx = 2
}
outcome = sample(c(1, 0), 1, prob = choice_probs[idx, ])
}
experience_decay_factor = rbeta(1, 1.2, 1.2)
payoff_decay_factor = rbeta(1, 1.2, 1.2)
inverse_temperature = rnorm(1, 0, 10)
choices = c("blue", "orange")
outcome_probs_blue = c(.8, .2) # blue is good
outcome_probs_orange = c(.2, .8) # orange is bad
outcomes = matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2, ncol = 2)
choice = c() # empty vector for choices
action_values = c()
experience_weights = c()
rm(action_values)
choice_values = c()
experience_weights = c()
outcomes = c() # empty vector for outcomes
ntrials = 50 # will do 50 before switching
i = 1
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, prob = c(.5, .5) )
choice
action_values[i] = .5 # because it is 50/50 at this point
choice_values[i] = .5 # because it is 50/50 at this point
experience_weights[i] = .5
i = 2
tmp_choice_values = values[c(max(which(choice == "blue")), max(which(choice == "orange")))]
choice_values[c(max(which(choice == "blue")), max(which(choice == "orange")))]
tmp_choice_values = choice_values[c(max(which(choice == "blue")), max(which(choice == "orange")))] #
tmp[is.na(tmp)] = .5
tmp_choice_value[is.na(tmp)] = .5
tmp_choice_values[is.na(tmp)] = .5 # just in case one has not assigned a value yet
tmp_choice_values[is.na(tmp_choice_value)] = .5 # just in case one has not assigned a value yet
tmp_choice_values[is.na(tmp_choice_values)] = .5 # just in case one has not assigned a value yet
tmp_choice_values
tmp_experience_weights = experience_weights[ c(max(which(choice == "blue")), max(which(choice == "orange")))  ]
tmp_experience_weights[is.na(tmp_experience_weights)] = .5
tmp_experience_weights
# convert each of these into the new weight value (i.e. apply update rule to last chosen, but pass through next value)
choice_value_for_update = tmp[ which(choices == choice[i-1])   ]
# convert each of these into the new weight value (i.e. apply update rule to last chosen, but pass through next value)
choice_value_for_update = tmp_choice_values[ which(choices == choice[i-1])   ]
choice_value_for_update
experience_weight_for_update = tmp_experience_weights[ which( choices == choice[i-1]) ]
new_choice_value = ewa_update( experience_decay_factor, payoff_decay_factor, experience_weight_for_update, choice_value_for_update)
new_choice_value = ewa_update( experience_decay_factor, payoff_decay_factor, experience_weight_for_update, choice_value_for_update, outcomes[i-1] )
new_choice_value
outcomes[i-1]
outcome
outcomes
outcome_probs = matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2, ncol = 2)
# get trial outcome
outcomes[i] = get_trial_outcome(choice[i], outcome_probs)
rm(list=ls())
###############################################################################################################################
# define model functions
ewa_update <- function(experience_decay_factor, payoff_decay_factor, last_experience_weight, last_choice_value, previous_outcome){
# given an experience decay factor, and  a payoff_decay _factor, compute the
# experience weight and the value for the choice, given the value, weight and outcome
# for the previous choice
experience_weight = last_experience_weight * experience_decay_factor + 1
choice_value = (last_choice_value * payoff_decay_factor * last_experience_weight + previous_outcome) / experience_weight
choice_value
}
action_selection <- function( choice_value, other_choice_value, inverse_temperature ){
# compute the probability of choice, given the value for that choice, the value for the other choice, and the
# inverse temperature parameter -
probability_choice = exp( beta*choice_value  ) / sum( exp( beta*choice_value ), exp( beta*other_choice_value ) )
probability_choice
}
get_trial_outcome <- function( choice, choice_probs ){
# sample the reward distribution lying beneath the choice,
if (choice == "blue"){
idx = 1
} else if (choice == "orange"){
idx = 2
}
outcome = sample(c(1, 0), 1, prob = choice_probs[idx, ])
}
experience_decay_factor = rbeta(1, 1.2, 1.2)
payoff_decay_factor = rbeta(1, 1.2, 1.2)
inverse_temperature = rnorm(1, 0, 10)
# define weightings of probability of good or bad outcome for each choice
# assuming choice between two stimuli, a 'blue' and an 'orange'
choices = c("blue", "orange")
outcome_probs_blue = c(.8, .2) # blue is good
outcome_probs_orange = c(.2, .8) # orange is bad
outcome_probs = matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2, ncol = 2)
choice = c() # empty vector for choices
choice_values = c()
experience_weights = c()
outcomes = c() # empty vector for outcomes
ntrials = 50 # will do 50 before switching
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, prob = c(.5, .5) )
i = 1
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, prob = c(.5, .5) )
choice_values[i] = .5 # because it is 50/50 at this point
experience_weights[i] = .5
# get trial outcome
outcomes[i] = get_trial_outcome(choice[i], outcome_probs)
outcomes
i = 2
# compute ewa & action selection
tmp_choice_values = choice_values[c(max(which(choice == "blue")), max(which(choice == "orange")))] # this gets the last value for each choice (don't worry about the warning msg)
tmp_choice_values[is.na(tmp_choice_values)] = .5 # just in case one has not assigned a value yet
tmp_experience_weights = experience_weights[ c(max(which(choice == "blue")), max(which(choice == "orange")))  ]
tmp_experience_weights[is.na(tmp_experience_weights)] = .5
# convert each of these into the new weight value (i.e. apply update rule to last chosen, but pass through next value)
choice_value_for_update = tmp_choice_values[ which(choices == choice[i-1])   ]
experience_weight_for_update = tmp_experience_weights[ which( choices == choice[i-1]) ]
new_choice_value = ewa_update( experience_decay_factor, payoff_decay_factor, experience_weight_for_update, choice_value_for_update, outcomes[i-1] )
new_choice_value
# assign to the choice values to be compared for action generation
tmp_choice_values[ which(choices == choice[i-1])   ] = new_choice_value
tmp_choice_values
# feed in to get action probabilities
this_trials_action_probs = c( action_selection( tmp_choice_values[1], tmp_choice_values[2], inverse_temperature ) )
rm(list=ls())
###############################################################################################################################
# define model functions
ewa_update <- function(experience_decay_factor, payoff_decay_factor, last_experience_weight, last_choice_value, previous_outcome){
# given an experience decay factor, and  a payoff_decay _factor, compute the
# experience weight and the value for the choice, given the value, weight and outcome
# for the previous choice
experience_weight = last_experience_weight * experience_decay_factor + 1
choice_value = (last_choice_value * payoff_decay_factor * last_experience_weight + previous_outcome) / experience_weight
choice_value
}
action_selection <- function( choice_value, other_choice_value, inverse_temperature ){
# compute the probability of choice, given the value for that choice, the value for the other choice, and the
# inverse temperature parameter -
probability_choice = exp( inverse_temperature*choice_value  ) / sum( exp( inverse_temperature*choice_value ), exp( inverse_temperature*other_choice_value ) )
probability_choice
}
get_trial_outcome <- function( choice, choice_probs ){
# sample the reward distribution lying beneath the choice,
if (choice == "blue"){
idx = 1
} else if (choice == "orange"){
idx = 2
}
outcome = sample(c(1, 0), 1, prob = choice_probs[idx, ])
}
experience_decay_factor = rbeta(1, 1.2, 1.2)
payoff_decay_factor = rbeta(1, 1.2, 1.2)
inverse_temperature = rnorm(1, 0, 10)
# define weightings of probability of good or bad outcome for each choice
# assuming choice between two stimuli, a 'blue' and an 'orange'
choices = c("blue", "orange")
outcome_probs_blue = c(.8, .2) # blue is good
outcome_probs_orange = c(.2, .8) # orange is bad
outcome_probs = matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2, ncol = 2)
# set up empty vectors to collect generated data
choice = c() # empty vector for choices
choice_values = c()
experience_weights = c()
outcomes = c() # empty vector for outcomes
ntrials = 50 # will do 50 before switching
i = 1
choice[i] = sample(choices, 1, prob = c(.5, .5) )
choice_values[i] = .5 # because it is 50/50 at this point
experience_weights[i] = .5
# get trial outcome
outcomes[i] = get_trial_outcome(choice[i], outcome_probs)
choice
choice_values
experience_weights
outcomes
i = 2
# compute ewa & action selection
tmp_choice_values = choice_values[c(max(which(choice == "blue")), max(which(choice == "orange")))] # this gets the last value for each choice (don't worry about the warning msg)
tmp_choice_values[is.na(tmp_choice_values)] = .5 # just in case one has not assigned a value yet
tmp_choice_values
tmp_experience_weights = experience_weights[ c(max(which(choice == "blue")), max(which(choice == "orange")))  ]
tmp_experience_weights[is.na(tmp_experience_weights)] = .5
# convert each of these into the new weight value (i.e. apply update rule to last chosen, but pass through next value)
choice_value_for_update = tmp_choice_values[ which(choices == choice[i-1])   ]
experience_weight_for_update = tmp_experience_weights[ which( choices == choice[i-1]) ]
new_choice_value = ewa_update( experience_decay_factor, payoff_decay_factor, experience_weight_for_update, choice_value_for_update, outcomes[i-1] )
# assign to the choice values to be compared for action generation
tmp_choice_values[ which(choices == choice[i-1])   ] = new_choice_value
tmp_choice_values
# feed in to get action probabilities
this_trials_action_probs = c( action_selection( tmp_choice_values[1], tmp_choice_values[2], inverse_temperature ) )
this_trials_action_probs
# feed in to get action probabilities
this_trials_action_probs = c( action_selection( tmp_choice_values[1], tmp_choice_values[2], inverse_temperature ), action_selection(  tmp_choice_values[2], tmp_choice_values[1], inverse_temperature   ) )
this_trials_action_probs
choice[i] = sample(choices, 1, prob = this_trials_action_probs)
choice
for (i in 1:ntrials)
if (i == 1){
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, prob = c(.5, .5) )
choice_values[i] = .5 # because it is 50/50 at this point
experience_weights[i] = .5
}
else {
# compute ewa & action selection
tmp_choice_values = choice_values[c(max(which(choice == "blue")), max(which(choice == "orange")))] # this gets the last value for each choice (don't worry about the warning msg)
tmp_choice_values[is.na(tmp_choice_values)] = .5 # just in case one has not assigned a value yet
tmp_experience_weights = experience_weights[ c(max(which(choice == "blue")), max(which(choice == "orange")))  ]
tmp_experience_weights[is.na(tmp_experience_weights)] = .5
# convert each of these into the new weight value (i.e. apply update rule to last chosen, but pass through next value)
choice_value_for_update = tmp_choice_values[ which(choices == choice[i-1])   ]
experience_weight_for_update = tmp_experience_weights[ which( choices == choice[i-1]) ]
new_choice_value = ewa_update( experience_decay_factor, payoff_decay_factor, experience_weight_for_update, choice_value_for_update, outcomes[i-1] )
# assign to the choice values to be compared for action generation
tmp_choice_values[ which(choices == choice[i-1])   ] = new_choice_value
# feed in to get action probabilities
this_trials_action_probs = c( action_selection( tmp_choice_values[1], tmp_choice_values[2], inverse_temperature ), action_selection(  tmp_choice_values[2], tmp_choice_values[1], inverse_temperature   ) )
choice[i] = sample(choices, 1, prob = this_trials_action_probs)
}
# get trial outcome
outcomes[i] = get_trial_outcome(choice[i], outcome_probs)
}
rm(list=ls())
###############################################################################################################################
# define model functions
ewa_update <- function(experience_decay_factor, payoff_decay_factor, last_experience_weight, last_choice_value, previous_outcome){
# given an experience decay factor, and  a payoff_decay _factor, compute the
# experience weight and the value for the choice, given the value, weight and outcome
# for the previous choice
experience_weight = last_experience_weight * experience_decay_factor + 1
choice_value = (last_choice_value * payoff_decay_factor * last_experience_weight + previous_outcome) / experience_weight
choice_value
}
action_selection <- function( choice_value, other_choice_value, inverse_temperature ){
# compute the probability of choice, given the value for that choice, the value for the other choice, and the
# inverse temperature parameter -
probability_choice = exp( inverse_temperature*choice_value  ) / sum( exp( inverse_temperature*choice_value ), exp( inverse_temperature*other_choice_value ) )
probability_choice
}
get_trial_outcome <- function( choice, choice_probs ){
# sample the reward distribution lying beneath the choice,
if (choice == "blue"){
idx = 1
} else if (choice == "orange"){
idx = 2
}
outcome = sample(c(1, 0), 1, prob = choice_probs[idx, ])
}
###############################################################################################################################
# now define parameters for simulation
###############################################################################################################################
# define the model parameters
# I have chosen to randomly sample them, given the prior distributions, taken from Ouder et al, 2013, Neuron
experience_decay_factor = rbeta(1, 1.2, 1.2)
payoff_decay_factor = rbeta(1, 1.2, 1.2)
inverse_temperature = rnorm(1, 0, 10)
# define weightings of probability of good or bad outcome for each choice
# assuming choice between two stimuli, a 'blue' and an 'orange'
choices = c("blue", "orange")
outcome_probs_blue = c(.8, .2) # blue is good
outcome_probs_orange = c(.2, .8) # orange is bad
outcome_probs = matrix(c(outcome_probs_blue, outcome_probs_orange), nrow = 2, ncol = 2)
# set up empty vectors to collect generated data
choice = c() # empty vector for choices
choice_values = c()
experience_weights = c()
outcomes = c() # empty vector for outcomes
ntrials = 50 # will do 50 before switching
###############################################################################################################################
# run simulation
###############################################################################################################################
for (i in 1:ntrials){
if (i == 1){
# assuming first choice is 50:50, 0 is blue, 1 is orange
choice[i] = sample(choices, 1, prob = c(.5, .5) )
choice_values[i] = .5 # because it is 50/50 at this point
experience_weights[i] = .5
}
else {
# compute ewa & action selection
tmp_choice_values = choice_values[c(max(which(choice == "blue")), max(which(choice == "orange")))] # this gets the last value for each choice (don't worry about the warning msg)
tmp_choice_values[is.na(tmp_choice_values)] = .5 # just in case one has not assigned a value yet
tmp_experience_weights = experience_weights[ c(max(which(choice == "blue")), max(which(choice == "orange")))  ]
tmp_experience_weights[is.na(tmp_experience_weights)] = .5
# convert each of these into the new weight value (i.e. apply update rule to last chosen, but pass through next value)
choice_value_for_update = tmp_choice_values[ which(choices == choice[i-1])   ]
experience_weight_for_update = tmp_experience_weights[ which( choices == choice[i-1]) ]
new_choice_value = ewa_update( experience_decay_factor, payoff_decay_factor, experience_weight_for_update, choice_value_for_update, outcomes[i-1] )
# assign to the choice values to be compared for action generation
tmp_choice_values[ which(choices == choice[i-1])   ] = new_choice_value
# feed in to get action probabilities
this_trials_action_probs = c( action_selection( tmp_choice_values[1], tmp_choice_values[2], inverse_temperature ), action_selection(  tmp_choice_values[2], tmp_choice_values[1], inverse_temperature   ) )
choice[i] = sample(choices, 1, prob = this_trials_action_probs)
}
# get trial outcome
outcomes[i] = get_trial_outcome(choice[i], outcome_probs)
}
outcomes
choices
choice
###############################################################################################################################
# make into a dataframe and save to wideform for further use
###############################################################################################################################
data <- data.frame(choices = choice, outcomes = outcomes)
write.csv(data, "example.csv")
setwd("~/Dropbox/learning_model_sims")
write.csv(data, "example.csv")
